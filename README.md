# ğŸ”¥ PySpark Learning Journey â€“ Data Engineering Track

This repository documents my hands-on journey to learn **Apache Spark** and its Python API **PySpark**, as part of my path to mastering **Data Engineering**.

---

## ğŸš€ Project: Job Listings Analyzer

A mini-project designed to explore and implement the **core concepts of Spark**, including:
- RDDs
- DataFrame API
- Spark SQL
- Structured Streaming (simulated)
- MLlib (Spark's machine learning library)

---

## ğŸ“ Directory Structure
  ```plaintext
.
â”œâ”€â”€ jobs.txt                 
â”œâ”€â”€ notebooks/               
â”‚   â”œâ”€â”€ 1_rdd_basics.ipynb
â”‚   â”œâ”€â”€ 2_dataframe_api.ipynb
â”‚   â”œâ”€â”€ 3_spark_sql.ipynb
â”‚   â”œâ”€â”€ 4_structured_streaming.ipynb
â”‚   â””â”€â”€ 5_mllib_model.ipynb
â”œâ”€â”€ stream_data/             
â”œâ”€â”€ output/                  
â””â”€â”€ README.md

```
---

## ğŸ“š Concepts Covered

### 1. ğŸ”§ Spark Core + RDD
- Loading data using `textFile()`
- Basic `map()` and `filter()` transformations
- Actions like `collect()` and `count()`

### 2. ğŸ“Š DataFrame API
- Creating and querying DataFrames
- Using `groupBy`, `agg`, and `select`
- Catalyst optimizer and lazy evaluation

### 3. ğŸ§  Spark SQL
- Creating temporary views
- Running SQL queries over Spark tables

### 4. âš¡ Structured Streaming (Simulated)
- Setting up `readStream` on CSV files
- Writing real-time output to console

### 5. ğŸ¤– MLlib (Machine Learning)
- Using `StringIndexer`, `VectorAssembler`
- Training a logistic regression model
- Predicting high-salary jobs

---

## âš™ï¸ Requirements

- Python 3.8+
- PySpark (`pip install pyspark`)
- Jupyter Notebook (recommended)
- Optional: VS Code or Google Colab

---

## ğŸ“ˆ How to Run

1. Clone the repository:
   ```bash
   git clone [https://github.com/your-username/pyspark-data-engineering](https://github.com/MutharasuArchunan13/Data-Engineering).git
   cd pyspark-data-engineering

